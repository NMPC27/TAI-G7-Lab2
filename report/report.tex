\documentclass{article}

\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}

\begin{document}

\title{
    Assignment 2 - Language detection using Copy Models for compression \\
    \large{Algorithmic Information Theory (2022/23) \\
    Universidade de Aveiro}
}

\author{
    Martinho Tavares, 98262, martinho.tavares@ua.pt \and
    Nuno Cunha, 98124, nunocunha@ua.pt \and
    Pedro Lima, 97860, p.lima@ua.pt
}

\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

The goal of this assignment is to implement a language detection system using the previously developed copy model from the last assignment.
The system is capable of detecting the language of a given text, if it is one of the languages used to train the model.
To implement this, a new version of the copy model was developed, called lang, which received a reference text to train the model and a target text to compress by copying from the reference.
The main idea is that if we use a reference text of a given language then we should get a better compression if the target text matches the provided language, since the same language patterns are to be expected, and so better copies can be made.
Then, we can identify the language of a given target text in terms of the language of the provided reference.
If using multiple references, we can comparatively identify a language, by checking which of them provided a better compression of the target text.

One of the other new features implemented in the lang model is the ability to use a finite-context model for the probabilities of non-hit symbols, instead of using the uniform and frequency probability distributions.

With the new model implemented, two other scripts were developed to test the performance of the model and obtain the language prediction in an entire text file or in specific sections of a text.
The first script is called find_lang, which receives a target text and uses all the existing references to predict the language of the target text, returning the language with the highest probability of being the target's language.
This is done by executing the lang model for each reference text and comparing the total information obtained for each language, returning the reference language with the lowest information.

The second script is called locate_lang, which receives a target text and, like the find_lang script, executes the lang model for each reference text, but instead of returning the language with the lowest information, it returns the language with the lowest information for each section of the target text.
This is done by saving which language has the lowest information for each step/character of the target text, and then checking the intervals where the language is the same, returning the language with the lowest information for each interval.

In this report we will present the methodology used to implement the lang model, the find_lang and locate_lang scripts, in more detail in section \ref{sec:methodology}, and the results obtained by testing the scripts with different target texts in section \ref{sec:results}.
And finally, in section \ref{sec:conclusion}, we will present the conclusions of this assignment.

\section{Methodology}
\label{sec:methodology}

For the development of the different components, we utilized different approaches.
The lang model was written in C++, extending and improving the copy model program developed in the previous assignment.

However, the find_lang and locate_lang scripts were developed in Python.
These scripts essentially perform many independent runs of the lang model using different references, and parse the output as they desire.
Since practically all of the heavy processing is performed by the lang model, we can afford the loss of performance that running a script in Python entails in turn for the much increased development simplicity.
In fact, in practice we noticed that most of the execution time spent on these scripts is in the lang model, as the result processing in Python is relatively simple.
For both scripts, parameters of the lang model can be specified.

Even then, the find_lang and locate_lang take too much time waiting for the results of the lang model.
Therefore, to ease this large bottleneck, we added support for multiprocessing to both of these scripts.
Since different lang models with different references can be run completely independently, without any need of interprocess communication, we can easily launch multiple processes.
The ideal option would be to integrate multithreading directly into the lang model program itself, in C++, which allows greater memory efficiency as the target text doesn't have to be stored in memory multiple times, one for each process.
However, we decided to simply opt for multiprocessing, since multithreading would heavily complicate the source code of the lang model, and also because multiprocessing provided a performance boost good enough for our purposes.
This allowed us to dedicate more time in other areas of the assignment, such as improving the lang model and the scripts.

If, while performing multiprocessing, any of the processes fails for some reason, then the script by default exits outputting the obtained error from the failed process.

In each of the following sections, we provide further development details on each of the components.

\subsection{Lang model}
\label{subsec:methodology_lang_model}

The developed lang model is heavily based on the copy model built for the first assignment.
The major change was adapting the existing program to train a copy model on a given reference text and only perform predictions on a given target text, instead of performing both training and predicting on a single message.
This allows estimation of the number of bits required to compress the target text using copies from the reference text.

For the lang program, the following changes were done from the first assignment's copy model:
\begin{itemize}
    \item UTF-8 support:
    We are using as references multiple kinds of languages.
    As such, the ASCII encoding used in the first assignment's copy model is not enough to correctly represent the symbols of every language, since not all alphabets fit in that encoding.
    The code worked perfectly for UTF-8 text, as it just effectively handled bytes.
    However, it could cutoff symbols, and it was harder to debug and visually analyze the behaviour of the copying.
    Therefore, we changed the code to support UTF-8 text, handling now symbols as wide chars instead of chars and strings as wstrings.
    
    \item Repeat-like wrapping for the past initialization:
    Since we want to start training/copying since the very first symbol in the reference/target message, we need to consider some kind of context when at that symbol, i.e. we need to somehow train and test the model using the first $k$ symbols, even if we require $k$ symbols of previous context for each of them, which doesn't naturally exist.
    For the first assignment, the strategy that we followed to solve this problem was to initiale this $k$-sized past with the most frequent symbol in the entire message.
    This was an easy solution to implement, but it's much less appropriate when applied for the specific use case of this assignment.
    Since we are dealing with natural language text, it doesn't make sense to incorporate $k$ repeated instances of a single letter, as that will definitely not occur for a reasonable value of $k$.
    For the first assignment we mainly dealt with a chromossome sequence, but in this case this issue becomes more obvious.

    Therefore, for the lang program we decided to initialize the $k$-sized past using the last $k$ symbols of the message.
    That is, we perform a repeat-like wrapping on the text.
    This way, we ensure the usage of a context in natural language for the first $k$ symbols.
    The difference between both approaches is illustrated in figure \ref{fig:lang_new_past}

    We have to handle the initialization of the past carefully for both the reference and target texts (note that they are in different character arrays in memory).
    When training on the reference text, we initialize its past with its last $k$ symbols.
    For the target text, we require some context for the first $k$ symbols before we start predicting.
    Therefore, we initialize in the same way, using the last $k$ symbols of the reference text.
    As such, it will be as if the target text was concatenated at the end of the reference text.
    
    \item Usage of \verb|string_view|:
    In the previous assignment, we used strings to represent the $k$-sized context/window as we advanced on the text.
    For the copy model itself, we had an hash table whose keys were these $k$-sized windows, i.e. $k$-sized strings.
    This had the disadvantage that, effectively, the entire content of the message being trained on would be reflected in the keys of this hash table.
    Since the message itself is already being stored in memory, the storage of keys in the hash table more than duplicates the amount of memory that the message's content ends up occupying.
    Additionally, advancing through the text involved altering the current $k$-sized window by appending the new symbol at the end and removing the oldest symbol at the beginning.
    This operation is not particularly efficient, especially when the string's content already exists and, in practice, all that is being done is the advancing of a pointer.

    Therefore, we decided to switch these strings to string views.
    String views just require a pointer to the beginning of a string and the string size.
    This allows the lang program to be more memory efficient by simply pointing to the message content that is already in memory, from which copies are made.
    Additionally, this enforces a fixed size for the strings, which for our case makes sense as the size is always a fixed value $k$.
    
    \item Usage of \verb|unordered_map|:
    Our previous copy model made heavy use of the \verb|map| data structure.
    However, this structure enforces order, which we don't take advantage of, and is not the approximation to an hash table that we desired, contrary to \verb|unordered_map|.
    As such, in the lang program we changed all uses of the \verb|map| structure to the \verb|unordered_map|, with noticeable improvement in performance.
    
    \item Removed ReadingStrategy:
    In the first assignment, we initially planned to evaluate different approaches to reading the message from disk: either entirely into disk, or in binary directly from disk in case we could not load the entire message into memory.
    We ended up not exploring different approaches other than storing all the content into memory, and so for the lang model we scrapped the abstraction of a reading strategy, incorporating the functionality that was in the respective classes directly into the lang copy model.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{./images/lang_new_past.png}
    \caption{Illustration of the new past initialization, with the difference between the previous version (above) and the new version (below). The english reference text is used as an example.}
    \label{fig:lang_new_past}
\end{figure}

Other than these changes, various bugfixes and slight optimizations were done on the previous copy model.
Due to the new approach of training on independent reference and target texts, the code had to be restructured and more responsibilities separated to ensure only training occurs on the reference and prediction is reserved only for the target.

For instance, after training on the reference we have an hash table of the counts of each symbol in the text.
These counts are required by the probability distributions, more specifically the \verb|FrequencyDistribution|.
However, the probability distributions have to take into account the \emph{target's} alphabet, not the reference's.
Therefore, after performing the initial pass over the target text to determine its alphabet, these counts are updated to remove any symbols that did not appear in the target text and include those that appeared in the target text but not in the reference.
These new symbols, that didn't previously have counts, will have their counts initialized with 1 to avoid reporting an infinite amount of information for them.
Do note that, after passing over the target text, the counts of the symbols both in the reference text and target text are not updated, i.e. only counts actually done on the reference text are used.
This is done because it doesn't make much sense to include the symbol counts on the target text when the objective of the program is to compress solely based on a model trained on a reference text.
Additionally, we mainly want to use this program to compare compression of one same target text using many different references, and so it's not relevant to include the target symbol counts since they would be the same between all runs.

Another point worth making is that most copy pointer managers, which are the copy model components responsible for determining which copy pointer to use at each context, do not perform differently every time the same context is seen in the target text.
This is because the array of copy pointers to choose from does not change while traversing the target text anymore, only during training on the reference.
As such, \verb|NextOldestCopyPointerManager| is the only manager whose behaviour changes for a given context while predicting on the target.

% TODO: profiling results? show the bottleneck?

\subsubsection{Verbose modes}
\label{subsubsec:methodology_lang_model_verbose}

The lang program was required to output the total number of bits estimated to be necessary to compress the target text based on the reference text.
To simplify output analysis by the find_lang script, we added the minimal verbose mode specified with the parameter \verb|-v o|, which simply prints to standard output nothing more than the total number of bits required for compression.

Additionally, for the locate_lang script, we needed to know what was the reported information content in bits for each symbol in the message, to correctly identify in which language was each text segment.
Therefore, we modified the previous ``machine'' verbose mode, specified with the parameter \verb|-v m|, which used to print the whole probability distribution in CSV format to standard output.
This output was previously saved to disk using shell redirection, and since the CSV format was very wasteful in terms of space a lot of disk memory was necessary for these files.

Since now we are only concerned with the probability of the actual symbol in the message, we only need one of those values of the probability distribution at every step.
As such, we opted to instead of printing the floating-point probability values to standard output, to write them directly in sequence to a file in binary format.
This allows for a densely-packed representation of the probability values over the message since no newlines or separators are used and the values are written in their full binary content instead of a character string representation in digits each occupying a byte, saving tremendously in space when compared with the previous assignment's approach.
We can then load these values in a highly efficient manner from the locate_lang script using the NumPy library.
Do note that these these floating-point values, declared as \verb|double|, are assumed to occupy 64 bits.

Since the new ``machine'' verbose mode outputs the probabilities to a binary file, we added the possibility to declare to which file they are written, by specifying it as an ``option parameter'': \verb|-v m:X|, where \verb|X| is the path of the file.
If only \verb|-m| is passed, then the output is written to ``lang.bin'' by default.
If the specified file already exists, then a prompt asking for ovewrite permission is presented.

\subsubsection{Finite-context model}
\label{subsubsec:methodology_lang_model_fcm}

We explored the possibility of using a finite-context model, or discrete time Markov chain, as the default distribution to report for the symbols that we did not predict at each step.
In this model, we try to estimate the probability that a symbol occurs after a given $k_f$-sized context by counting occurrences of the symbol after that context in the past.
We allowed this model to have an arbitrary order, i.e. the size $k_f$ of the context used.
We can specify a $k_f \leq k$, where $k$ is the size of the context in the copy model itself.

Since we already had a family of classes representing the reported probability distributions, as subclasses of the \verb|BaseDistribution| class, then we created the finite-context model as a subclass as well, named \verb|FiniteContextDistribution|.
Implementation involved keeping track of counts of symbols after $k_f$-sized subcontexts we saw while training on the reference text.
For that, we used an hash table with the context as the keys and the symbol counts as the values.
The symbol counts themselves were also implemented using hash tables, with the symbols as the keys and the counts as the values.
These tables are updated at every training step on the reference.

After training, we initialize the internal distribution variable, which contains the probability distribution that is reported at each step, with zeros for each symbol of the target alphabet.
Then, for every hash table of counts in the hash table of subcontexts, we remove all the counts of the symbols that are not in the alphabet, since they won't be used.

Then, at every single prediction step, we update the internal distribution variable to store the probabilities of every symbol based on the obtained symbol counts for the current $k_f$-sized subcontext.
The probabilities are obtained using the following formula:

$$
P(e|c) = \frac{N(e|c) + \alpha}{\sum_{s \in \Sigma}{N(s|c)} + \alpha |\Sigma|},
$$

where $e$ is the symbol of which we want to calculate the probability of occurring, $c$ is the subcontext used, $\Sigma$ is the target alphabet, $N(x|y)$ is the count of symbol $x$ after sub $y$ and $\alpha$ is a smoothing parameter to avoid probabilities of 0.

In case no counts were obtained for the current $k_f$-sized subcontext, then all counts are treated as 0 and the reported distribution is a uniform distribution.

This implementation requires recalculating, at each prediction step, the probabilities of all symbols at each subcontext found in the target text.
If we found the same subcontext multiple times, then the distribution would be recalculated at each one of them.
It can be argued that this is wasteful, and that the distribution can be cached somehow.
In fact, we attempted to calculate all probability distributions for each subcontext in the main hash table beforehand, instead of keeping track of the counts.
However, this is extremely expensive in terms of memory usage, since normally we only keep counts of a few symbols for each subcontext, but with this approach we need to store counts for all symbols of the target alphabet.
Even after trying to use arrays instead of hash tables to store the probability values, the memory usage was too high (upwards of gigabytes).
Surprisingly, as well, the performance didn't even increase.
Since the target text was usually smaller than the reference text, most of the work was being performed in the preemptive calculation of the probability distributions, most of which were never going to be queried for.
As such, we opted for the lazy calculation of the probability distributions, since in practice the performance footprint was not very noticeable and the memory usage was much less.

\subsection{Find lang}
\label{subsec:methodology_find_lang}

The find lang script was required to, given a target text and a bank of references, determine in which language it was written.
This is done by running the lang program for each combination of the target text and a reference text from the bank, getting the reported total information content in bits using the minimal verbose mode \verb|-v o|, and reporting the reference text whose reported value was the lowest.

The script doesn't take into account its overall performance to, for example, indicate that it doesn't know which language the target text is in, i.e. it always says that the target text is one of the supplied references' languages.

We also added a metric to the script's output, called \textit{confidence}.
Confidence dictates how sure is the script that the reported language is actually the target text's language.
This metric only takes into account the relative performance of the lang program with the different references, and not the performance of the lang program overall.
Confidence simply measures how confused the script is in the reported language.
We essentially compare the lowest reported information value with the second lowest information value.
If these values are very different between each other, then we can be more sure that the language that reported the minimum is actually the target text's language (within the known references at least).
However, if they are very similar, then there is confusion between at least two languages, and we can't be very sure that the reported minimum is certainly the target language (this can happen, for instance, for very similar languages).

The confidence value $C$ is calculated using the following formula:

$$
C = \sqrt[3]{1 - \frac{m_2}{m_1}^3},
$$

where $m_k$ is the $k$-th minimum information value and $0 \leq C < 1$.
This formula, for instance, reports a confidence of $\approx 95.65\%$ if the second minimum is twice as large as the first minimum, $\approx 88.95\%$ if the second minimum is $50\%$ larger than the first minimum, but only $\approx 62.89\%$ if the second minimum is $10\%$ larger than the absolute minimum.
If the reported information is the same between both minimums, then confidence is $0\%$.
We opted for this formula since it has very accelerated growth in the beginning ($m_2$ slightly larger than $m_1$), allowing us to assert that as long as a small enough distance between both minimums is ensured, then we can be confident about the reported language.

\subsection{Locate lang}
\label{subsec:methodology_locate_lang}

The locate_lang script is comparatively more complex than find_lang.
Instead of identifying the language of the target overall, it attempts to identify which segments of text are written in one of the languages from the bank of references.
Additionally, locate_lang can also identify segments of text on which it can't assign a language from the references, and as such takes into account the absolute compression performance of the lang program.

To do this, analysis is done on the reported information content at each position in the target text.
We require this data for each reference.
This is obtained by running the lang program for each reference using the machine verbose mode \verb|-v m|, which outputs all this information into a binary file in disk.
After all these results are saved, they are read from disk and analysis is performed by the script.

Contrary to the find_lang script, the locate_lang script caches the results of the lang model executions.
This was done to speed up analysis since the processing of locate_lang itself contains many different parameters, and so we can avoid recalculating all necessary information in every run of the script if we only want to adjust the result processing and not the lang model parameters.
This contrasts with the find_lang script, whose processing doesn't have any parameters, requiring only parameters to the lang model.
The cached results keep track of which target file, which references and which parameters were passed to the lang program.

The information at each target step for every reference is saved in a NumPy matrix $M$, with a row for each language reference and a column for each position in the target text.
This matrix is then manipulated to report the most likely row index corresponding to the probable language at each column, forming a row vector.
From this vector, we segment the target indices into continuous groups, returning at which target position each group starts and to which language it belongs to.

Since the information at the rows is very erratic and has many spikes due to the behaviour of the copy model, where copies are only done sometimes and not for very long, the data at each row is filtered using a low-pass filter to smoothen it.
The low-pass filter was implemented using the Fast Fourier Transform (FFT).
The idea was to get the frequencies that compose the data, downscale the higher frequencies, and then rebuild the signal using the Inverse Fast Fourier Transform (IFFT).
For downscaling, we used the exponential function $e^{-\beta|x|}$, where $x$ is the frequency and $\beta$ is the frequency dropoff, which dictates how strongly should the higher frequencies be filtered.
This function is centered at $x=0$, which allows positive and negative frequencies close to 0 (the lowest) to have an higher multiplier, but the farther they get from 0 the lower the multiplier is.
The downscaling is therefore implemented by multipliying the exponential downscaling function with the amplitudes of the frequencies of the data.
An example of the frequencies composing the target data and the corresponding downscaling function is presented in figure \ref{fig:low_pass_filter}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{./images/low_pass_filter.png}
    \caption{Example of the frequencies composing the compressed information of target ``all_languages_random.txt'' trained on the greek reference, with default lang model parameters and finite-context distribution with $k_f=6$. The exponential downscaling function applied to the frequencies is also shown on the right plot.}
    \label{fig:low_pass_filter}
\end{figure}

With the matrix $M$ representing the information at each step for every reference, we determine for each column, i.e. position in the target text, which row provides the minimum amount of information, that is which reference compressed the target symbol the best.
However, we don't consider all kinds of minimum values.
Since for locate_lang we want to identify sections of text that we deem unknown, i.e. they likely aren't related to any of the references, then we have to consider some kind of thresholds that the minimum values have to surpass to be truly accepted.

For this we considered two thresholds:
\begin{itemize}
    \item minimum threshold: the absolute minimum value should not be larger than a given fraction of the second minimum. This threshold was implemented to disregard sections where there is confusion between more than two references. This idea is similar to the \textit{confidence} metric calculated for find_lang in section \ref{subsec:methodology_find_lang}.
    \item static threshold: the absolute minimum value should not be larger than a given static threshold in bits. With this threshold, we disregard sections of text where the compression was not sufficient enough to consider that a reference truly represented the target language.
    This threshold is defined in terms of the cardinality of the target alphabet.
    More precisely, the static threshold $S$ in bits is given by the formula:

    $$
    S = \frac{\log_2 |\Sigma|}{\gamma},
    $$

    where $\Sigma$ is the target alphabet and $\gamma$ is a real positive value.
    This way, we define the static threshold as a fraction of a baseline compression, in which the reported probability distribution at each step is a uniform distribution with each symbol having $\log_2 |\Sigma|$ bits of information.
    As such, if we set for instance $\gamma = 1$, then the threshold would be extremely forgiving, since as long as a minimum amount of compression is performed the references are considered.
    However, as $\gamma$ increases, the threshold becomes more demanding, and requires the copy model to perform well enough to fall under the threshold.
    We set $\gamma$ to a value that we consider appropriate enough for our model's performance, such that if when faced with an unkown language the model's performance should be closer to a uniform distribution and above this static threshold.
\end{itemize}

In the end, the following processing arguments are accepted:
\begin{itemize}
    \item \verb|minimum-threshold|: fraction of the second minimum below which the absolute minimum has to be so that it is considered.
    \item \verb|static-threshold|: denominator $\gamma$ that defines the static threshold in terms of the target alphabet's cardinality
    \item \verb|frequency-filter|: the frequency dropoff of the low pass filter
    \item \verb|fill-unknown|: make an effort to consider unknown language segments as being of one of the previously identified languages, using forward and backward filling.
    \item \verb|labeled-output|: print the target text, with segments colored depending on the identified language.
    \item \verb|plot|: plot the entire matrix $M$ after applying the low pass filter, along with the static threshold.
    \item \verb|save-result|: save the plot of matrix $M$ to the specified file instead of showing it (only relevant if \verb|plot| was passed).
\end{itemize}

\section{Results}
\label{sec:results}

% TODO: talk about the compression results as well: why is the number of reported total bits and mean bits per symbol so low? It's probably because we are not accounting for the space required to store the reference text, which was necessary to compress the target to an absurdly low number of bits.

To test the performance of the model and the scripts implemented in this assignment, we used a set of reference texts covering 20 different languages,
these being: Bulgarian, Czech, Danish, German, Greek, English, Spanish, Estonian, Finnish, French, Hungarian, Italian, Lithuanian, Latvian, Dutch, Polish, Portuguese, Romanian, Slovak, Slovenian and Swedish.
For the target texts, we used a set of texts in the same languages as the reference texts, plus a chinese text, but with different content, which were mainly used to test the find_lang script.
And finally to test the locate_lang script, we combined the previous target files to a single file named \textit{all_languages.txt} and used it as a target text, and also used a small target text called \textit{dog.txt}
which also contained some languages mixed in, including chinese, which is not present in our reference files.

\subsection{Find lang}
\label{subsec:results_find_lang}

To obtain the accuracy of the find_lang script, we used the individual language texts and compared the result of the script with the actual language of the text.
Two runs were performed for each language file, one which executed the lang model using its default parameters, such as using a frequency base distribution,
and another one which used the finite-context model, instead of the other two options for probability distributions, along with the other default parameters.

For the first run, with default parameters, the script always correctly identified the language of the text, with an accuracy of 1.
\begin{equation}
    \label{eq:find_lang_default_accuracy}
    Accuracy = \frac{correct}{total} = \frac{20}{20} = 1
\end{equation}

For the second run, with the finite-context model, the script also correctly identified the language for all the texts used, with an accuracy of 1.
\begin{equation}
    \label{eq:find_lang_finite_context_accuracy}
    Accuracy = \frac{correct}{total} = \frac{20}{20} = 1
\end{equation}

To differentiate between the two approaches, we also calculated a confidence value. The way this value is calculated is through equation \ref{eq:find_lang_confidence}.

The following tables show the confidence results of the find_lang script for the default parameters and the finite-context model, respectively.

% TODO: add chinese?

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Language & Confidence \\
        \hline
        Bulgarian & 99.2\% \\
        Czech & 59.2\% \\
        Danish & 58.2\% \\
        German & 60.6\% \\
        Greek & 98.9\% \\
        English & 50.5\% \\
        Spanish & 46.4\% \\
        Estonian & 53.5\% \\
        Finnish & 41.2\% \\
        French & 45.8\% \\
        Hungarian & 67.1\% \\
        Italian & 45.1\% \\
        Lithuanian & 64.7\% \\
        Latvian & 74.8\% \\
        Dutch & 35.9\% \\
        Polish & 72.2\% \\
        Portuguese & 44.7\% \\
        Romanian & 74.6\% \\
        Slovak & 36.1\% \\
        Slovenian & 30.2\% \\
        Swedish & 59.8\% \\
        \hline
    \end{tabular}
    \caption{Confidence results for the find_lang script with default parameters}
    \label{tab:find_lang_default_confidence}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Language & Confidence \\
        \hline
        Bulgarian & 93.1\% \\
        Czech & 77.3\% \\
        Danish & 85.5\% \\
        German & 92.1\% \\
        Greek & 93.9\% \\
        English & 92.3\% \\
        Spanish & 86.4\% \\
        Estonian & 90.4\% \\
        Finnish & 89.7\% \\
        French & 91.7\% \\
        Hungarian & 92.9\% \\
        Italian & 90.2\% \\
        Lithuanian & 90.4\% \\
        Latvian & 89.8\% \\
        Dutch & 91.1\% \\
        Polish & 89.4\% \\
        Portuguese & 82.7\% \\
        Romanian & 91.2\% \\
        Slovak & 77.8\% \\
        Slovenian & 86.7\% \\
        Swedish & 89.2\% \\
        \hline
    \end{tabular}
    \caption{Confidence results for the find_lang script with finite-context model}
    \label{tab:find_lang_finite_context_confidence}
\end{table}

By comparing the confidence results of the two approaches, we can see that although both approaches correctly identified the language of the text,
the default parameters approach had a much lower confidence value than the finite-context model approach, some of them being even lower than 50\%.

The reason for this is that the default parameters approach uses a frequency base distribution, which is not very good at identifying the language of a text,
because it does not take into account the order of the characters in the text, which is very important for some languages.

After implementing the confidence value, we also tested the script with a text of a language that was not in the reference files, in this case, chinese.
Due to the fact that the script always returns a language, the way to evaluate the accuracy of the script is by checking if the returned language has a very low confidence value.

For the default parameters approach, the script returned the language as hungarian, with a confidence value of 37.7\%, which is low, but not low enough considering the other confidence values and
the fact that chinese has a completely different alphabet than any of the other languages present in the reference files.
In contrast, the finite-context model approach returned the language as swedish, but with a confidence value of 0.0\%, which is the lowest possible value, and the correct result
for a language that is not in the reference files.

With these results we can conclude that the finite-context model approach is better than the default parameters approach and has a really good accuracy and confidence value.

\subsection{Locate lang}
\label{subsec:results_locate_lang}

\dots

For all graphs showing the reported information at each target position, the frequency dropoff used was 1000.

\subsubsection{Base probability distribution - Uniform distribution}
\label{subsubsec:results_locate_lang_uniform_distribution}

% CUNHA
By using this approach, we can see very clearly the different languages in figure \ref{fig:all_languages_p_u} of file \textit{all_languages.txt},
because the entropy of the correct language is much lower than all the others.
But by using this approach, we have more difficulty detecting unknown languages (languages that are not in the references).
Because even if we define the static threshold at the lowest entropy point of all the correct languages,
it will also detect the last chunk of text as French, which is wrong, the last chunk of text in the file is Chinese.

For \textit{all_languages_random.txt}, we can distinguish the languages in the figure \ref{fig:all_languages_random_p_u}.
We can see the valleys very clearly and infer the language using that information.
The same difficulty is seen in this figure: we can't define a static threshold without affecting some of the real languages.
(We can't draw a horizontal line that is above all the reference languages and under the Chinese one.)

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-p_u.png}
    \caption{Entropy of each language for "all_languages.txt",uniform distribution}
    \label{fig:all_languages_p_u}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-p_u.png}
    \caption{Entropy of each language for "all_languages_random.txt",uniform distribution}
    \label{fig:all_languages_random_p_u}
\end{figure}

\subsubsection{Base probability distribution - Frequency distribution}
\label{subsubsec:results_locate_lang_frequency_distribution}

Using this approach, we can see the different languages in figure \ref{fig:all_languages_p_f} of file \textit{all_languages.txt}, although it produces a very different plot
compared to the uniform distribution approach.

Its still possible to distinguish the languages in figure \ref{fig:all_languages_random_p_f} of file \textit{all_languages_random.txt}, but it is harder to do so,
and another difference is between bulgarian and greek languages to the other languages. These have a much higher entropy than the others, due to the fact that these
languages use a completely different alphabet than the others.

A downside of this approach is that the static threshold line becomes higher, which means most of the languages will pass the threshold, even in their peak entropy values,
at least in the case of the \textit{all_languages.txt} file. For the \textit{all_languages_random.txt} file, the peaks are higher due to each line in the file being a random language.

Still, even with this downside, the script still manages to correctly identify the language of the sections, albeit with less accuracy in the entire interval on the random file,
due to the constant change of languages.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-p_f.png}
    \caption{Entropy of each language for "all_languages.txt",frequency distribution}
    \label{fig:all_languages_p_f}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-p_f.png}
    \caption{Entropy of each language for "all_languages_random.txt",frequency distribution}
    \label{fig:all_languages_random_p_f}
\end{figure}

\subsubsection{Base probability distribution - Finite-context model}
\label{subsubsec:results_locate_lang_first_order_fcm}

To test this approach, various sub parameters were tested, such as the size of the context and the smoothing parameter used to calculate the probability distribution.

For the context size, $k$ was tested with 3, 6 and 9, and for the smoothing parameter, $\alpha$, the values tested were 1, 10 and 100.

The results for both files are shown in figures \ref{fig:all_languages_p_c} and \ref{fig:all_languages_random_p_c}.

By verifiying the results for the context size in figure \ref{fig:all_languages_p_c}, we can see that the finite-context model produces a much better result than the other approaches,
because it is able to clearly distinguish the correct language (the minimum value), with the second best candidate.
When increasing $k$, this difference becomes even more clear, as seen in figures \ref{fig:all_languages_p_c:1:3} and \ref{fig:all_languages_p_c:1:6},
although it increases the general entropy value per language, which makes the threshold line to low to detect the correct language in each section.

This problem can also be seen in the random file, in figure \ref{fig:all_languages_random_p_c}, where the threshold line is too low to detect the correct language in each section when the context size
becomes $6$ or $9$.
Similarly, it is also possible to verify, although not as clearly due to the random nature of the file, that the correct language becomes more clear when increasing the context size.

This is due to the threshold not being tailor made for each file, but instead being a static value, which is not ideal, although it is clearly visible what the correct language is
in each section due to the difference in entropy values between the correct language and the second best candidate.

Now for the smoothing parameter, $\alpha$, we can see that the results are very similar for all the values tested, as seen in figures \ref{fig:all_languages_p_c:1:3}, \ref{fig:all_languages_p_c:10:3},
with the biggest difference being more noticiable in the random file, in figure \ref{fig:all_languages_random_p_c:100:3}, where the static threshold line is too low to detect the correct language.
This is due to the fact that increasing the smoothing parameter increases the entropy value of each language, which makes the threshold line too low to detect the correct language in each section.

Through these results, the best value for \alpha was $10$ and for the context size was $3$, because these helped to distinguish the correct language from the second best candidate, while not increasing the entropy value too much to make the threshold line too low.

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:1:3.png}
        \end{center}
        \caption{$k = 3$}
        \label{fig:all_languages_p_c:1:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:1:6.png}
        \end{center}
        \caption{$k = 6$}
        \label{fig:all_languages_p_c:1:6}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:1:9.png}
        \end{center}
        \caption{$k = 9$}
        \label{fig:all_languages_p_c:1:9}
    \end{subfigure}
    
    \caption{Entropy of each language for "all_languages.txt",finite-context model by changing $k$}
    \label{fig:all_languages_p_c:k}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:1:3.png}
        \end{center}
        \caption{$k = 3$}
        \label{fig:all_languages_random_p_c:1:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:1:6.png}
        \end{center}
        \caption{$k = 6$}
        \label{fig:all_languages_random_p_c:1:6}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:1:9.png}
        \end{center}
        \caption{$k = 9$}
        \label{fig:all_languages_random_p_c:1:9}
    \end{subfigure}
    
    \caption{Entropy of each language for "all_languages_random.txt",finite-context model by changing $k$}
    \label{fig:all_languages_random_p_c:k}
\end{figure}


\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:1:3.png}
        \end{center}
        \caption{$\alpha = 1$}
        \label{fig:all_languages_p_c:1:3_again}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:10:3.png}
        \end{center}
        \caption{$\alpha = 10$}
        \label{fig:all_languages_p_c:10:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages/-p_c:100:3.png}
        \end{center}
        \caption{$\alpha = 100$}
        \label{fig:all_languages_p_c:100:3}
    \end{subfigure}
    
    \caption{Entropy of each language for "all_languages.txt",finite-context model by changing $\alpha$}
    \label{fig:all_languages_p_c:alpha}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:1:3.png}
        \end{center}
        \caption{$\alpha = 1$}
        \label{fig:all_languages_random_p_c:1:3_again}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:10:3.png}
        \end{center}
        \caption{$\alpha = 10$}
        \label{fig:all_languages_random_p_c:10:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../results/all_languages_random/-p_c:100:3.png}
        \end{center}
        \caption{$\alpha = 100$}
        \label{fig:all_languages_random_p_c:100:3}
    \end{subfigure}
    
    \caption{Entropy of each language for "all_languages_random.txt",finite-context model by changing $\alpha$}
    \label{fig:all_languages_random_p_c:alpha}
\end{figure}

\subsubsection{Copy pointer repositioning - Oldest pointer first}
\label{subsubsec:results_locate_lang_oldest_pointer_first}

To see the effect of the pointer repositioning, we tested the different approaches when using a frequency distribution, which was the default parameter when testing.
So these results will be mostly similar we the results shown for the frequency distribution section.

In this approach we can still distinct the different languages, but not as clearly as some of the previous approaches.
In the figure \ref{fig:all_languages_r_o} we can see that the entropy of the languages are mostly the same, excluding Greek and Bulgarian.
One of the positives of this approach is that we can identify much more easily an unknown language (languages that are not in the references) than in some of the previous approaches
For the “all_languages_random.txt” file we can see in the figure \ref{fig:all_languages_r_o}, that the same behavior is seen, but now with slightly more differences between languages.

In contrast to the previous assignment, the lang model is fully trained before compressing the target file. This means that the pointer chosen for each pattern will always be the same.
This means that the pointer repositioning will not have much effect on the entropy of the target file, since the pointer will always be the same.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-r_o.png}
    \caption{Entropy of each language for "all_languages.txt",oldest pointer first}
    \label{fig:all_languages_r_o}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-r_o.png}
    \caption{Entropy of each language for "all_languages_random.txt",oldest pointer first}
    \label{fig:all_languages_random_r_o}
\end{figure}

\subsubsection{Copy pointer repositioning - Newest pointer first}
\label{subsubsec:results_locate_lang_newest_pointer_first}

Like in the older pointer approach, we can still distinct the different languages, using the newest pointer.
In both the \ref{fig:all_languages_r_n} and \ref{fig:all_languages_random_r_n}, we can see that the behavior is extremely similar to the older pointer approach,
because the pointer chosen for each pattern will always be the same, just like the older pointer approach.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-r_n.png}
    \caption{Entropy of each language for "all_languages.txt",newer pointer first}
    \label{fig:all_languages_r_n}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-r_n.png}
    \caption{Entropy of each language for "all_languages_random.txt",newer pointer first}
    \label{fig:all_languages_random_r_n}
\end{figure}

\subsubsection{Copy pointer repositioning - Most common prediction}
\label{subsubsec:results_locate_lang_most_common_prediction}

Just like the previous approaches, with this strategy, the pointer chosen for each pattern will always be the same.
But in this case, the pointer chosen will be the one with the most common prediction.

Also, like the previous approaches, we can still distinct the different languages, though the results are still identical, as we can see in the figures \ref{fig:all_languages_r_m} and \ref{fig:all_languages_random_r_m}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-r_m.png}
    \caption{Entropy of each language for "all_languages.txt",Most common prediction}
    \label{fig:all_languages_r_m}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-r_m.png}
    \caption{Entropy of each language for "all_languages_random.txt",Most common prediction}
    \label{fig:all_languages_random_r_m}
\end{figure}

\subsubsection{Copy pointer repositioning - Most common prediction bounded}
\label{subsubsec:results_locate_lang_most_common_prediction_bounded}

This version of the most common prediction strategy, is the same as the previous one, but with a bound on the number of pointers that can be chosen.
For this test we used the best value for the size of the bound that we verified in the last assignment, which was $100$.

Similarly to the previous repositioning strategies, because the model is trained before the compression, the results are identical to the previous approaches.
These results can be seen in the figures \ref{fig:all_languages_r_c} and \ref{fig:all_languages_random_r_c}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-r_c:100.png}
    \caption{Entropy of each language for "all_languages.txt",Most common prediction bounded}
    \label{fig:all_languages_r_c}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-r_c:100.png}
    \caption{Entropy of each language for "all_languages_random.txt",Most common prediction bounded}
    \label{fig:all_languages_random_r_c}
\end{figure}

\subsubsection{Copy pointer threshold - Static threshold}
\label{subsubsec:results_locate_lang_static_threshold}

Finnaly, we also tested the different threshold strategies, starting with the static threshold.

In this strategy, the threshold is set to a fixed value, which in this case is $0.5$ because it was the one which produced the lowest entropy values in the previous assignment.

In the figures \ref{fig:all_languages_t_n} and \ref{fig:all_languages_random_t_n}, we can see that the results are identical to the previous approaches.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-t_n:0.5.png}
    \caption{Entropy of each language for "all_languages.txt",Static threshold}
    \label{fig:all_languages_t_n}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-t_n:0.5.png}
    \caption{Entropy of each language for "all_languages_random.txt",Static threshold}
    \label{fig:all_languages_random_t_n}
\end{figure}

\subsubsection{Copy pointer threshold - Successive fails threshold}
\label{subsubsec:results_locate_lang_successive_fails_threshold}

In this strategy, the threshold is set to a value that is calculated based on the number of successive fails that the model has.
In this case, we used the value that we used in the previous assignment that produced better results, which was $9$.

Interestingly, in the figures \ref{fig:all_languages_t_f} and \ref{fig:all_languages_random_t_f}, we can see that the results, although similar to the use of the static threshold,
the entropy values are slightly higher when observing the minimum entropy values, corresponding to the predicted language for the section.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-t_f:9.png}
    \caption{Entropy of each language for "all_languages.txt",Successive fails threshold}
    \label{fig:all_languages_t_f}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-t_f:9.png}
    \caption{Entropy of each language for "all_languages_random.txt",Successive fails threshold}
    \label{fig:all_languages_random_t_f}
\end{figure}

\subsubsection{Copy pointer threshold - Rate of change threshold}
\label{subsubsec:results_locate_lang_rate_of_change_threshold}

Finnaly, we tested the rate of change threshold strategy with the value $0.01$, which is the one that produced the best results in the previous assignment.

By observing the figures \ref{fig:all_languages_t_c} and \ref{fig:all_languages_random_t_c}, we can see that the difference between the entropy values of the minimun entropy
and second minimun entropy is a bit higher than the previous threshold strategies, which means that the model is more confident in the prediction of the language of the section.

This makes this strategy the best one when choosing the threshold to switch pointers.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages/-t_c:0.01.png}
    \caption{Entropy of each language for "all_languages.txt",Rate of change threshold}
    \label{fig:all_languages_t_c}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{../results/all_languages_random/-t_c:0.01.png}
    \caption{Entropy of each language for "all_languages_random.txt",Rate of change threshold}
    \label{fig:all_languages_random_t_c}
\end{figure}

\subsubsection{Variable reference size}

We also analyzed the effect of the reference's size on the language classification.

\dots

\section{Conclusion}
\label{sec:conclusion}

\section{References}
\bibliography{refs}
\bibliographystyle{IEEEtran}

\end{document}
